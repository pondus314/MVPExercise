{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise: Image Classification Competition - updated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pondus314/MVPExercise/blob/master/Exercise_Image_Classification_Competition_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjcSiC4l-Un7"
      },
      "source": [
        "# **Warm Up Exercise (5%)**\n",
        "For this exercise, we will provide a demo code showing how to train a network on a small dataset called FashionMinst. Please go through the following tutorials first. You will get a basic understanding about how to train an image classification network in pytorch. You can change the training scheme and the network structure. Please answer the following questions then.\n",
        "\n",
        "Requirement:\n",
        "1. (1 point) Change the learning rate and train for 10 epochs. Fill this table:\n",
        "\n",
        "|Lr|Accuracy|\n",
        "|---|---|\n",
        "|1   |      |\n",
        "| 0.1|          |\n",
        "|0.01|         |\n",
        "|0.001  |        |\n",
        "\n",
        "\n",
        "2. (2 point) Build a deeper/ wider network. Report the accuracy and the parameters for each structure.\n",
        "\n",
        "|Structures|Accuracy|Parameters|\n",
        "|---|---|---|\n",
        "|Base   |      ||\n",
        "|Deeper|          ||\n",
        "|Wider|         ||\n",
        "\n",
        "3. (1 point) Calculate the mean of the gradients of the loss to all trainable parameters. Plot the gradients curve for the first 100 training steps. What are your observations? Note that this gradients will be saved with the training weight automatically after you call loss.backwards(). Hint: the mean of the gradients should be decreased.\n",
        "\n",
        "\n",
        "4. (1 point) Calculate the mean of the gradients for each layer in the first step. What are your observations?  Hint: According to the chain rule, the gradients for different layers will at different scales. \n",
        "\n",
        "For more exlanation of q3 and q4, you could refer to the following simple instructions: https://colab.research.google.com/drive/1XAsyNegGSvMf3_B6MrsXht7-fHqtJ7OW?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9oD1KU_J18F"
      },
      "source": [
        "#### Tutorial Code\n",
        "####PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset. \n",
        "#####Dataset stores samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download training data from open datasets. \n",
        "##Every TorchVision Dataset includes two arguments: \n",
        "##transform and target_transform to modify the samples and labels respectively.\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gOUO_v8KYmO"
      },
      "source": [
        "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset and supports automatic batching, sampling, shuffling, and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2kpqgtlKaor"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLZSoxZPKdH5"
      },
      "source": [
        "To define a neural network in PyTorch, we create a class that inherits from **nn.Module**. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7e0v5SoKgZU"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wPbhPTTKv8c"
      },
      "source": [
        "###Define the loss function and the optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6EK3TdGK47C"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf9XKCPEK4RC"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIo61xYFK9rK"
      },
      "source": [
        "##Define a test function\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOMqblWOLEwl"
      },
      "source": [
        "#Train and test the model\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrP6Y_wEZRR1"
      },
      "source": [
        "# **Image Classification Competition (15%)**\n",
        "For this competition, we will use the CIFAR10 dataset.\n",
        "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
        "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of\n",
        "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
        "\n",
        "We provide a baseline by the following steps:\n",
        "\n",
        "1. Load and normalize the CIFAR10 training and test datasets using\n",
        "   ``torchvision``\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data\n",
        "\n",
        "The following tricks could be considered:\n",
        "-------\n",
        "1. Advanced training scheme: learning rate, optimizer, batch size and max epochs\n",
        "2. New loss functions\n",
        "3. Data augmentation\n",
        "4. Add new layers in the network: batch normalization, drop out, residual layers, attention block, and other new blocks.\n",
        "\n",
        "Your code should be modified from the provided baseline. A pdf report is required to explain the tricks you employed, and the imporvements they achieved.\n",
        "Marking rules:\n",
        "-------\n",
        "We will mark the competition based on the final test accuracy on 1000 images and your report.\n",
        "\n",
        "Final mark = acc_mark + report mark + bonus mark\n",
        "\n",
        "###Acc_mark:\n",
        "\n",
        "|Accuracy|Mark|\n",
        "|---|---|\n",
        "| >60%|          4|\n",
        " |>70%|         6|\n",
        " |>80%  |        8|\n",
        "| >90%    |      10|\n",
        "\n",
        "\n",
        "###Report mark:\n",
        "1. Employed more than three tricks: 3 points\n",
        "2. Abalation studies for each tricks: 2 points\n",
        "\n",
        "###Bouns mark:\n",
        "1. Top three results: 1 mark\n",
        "2. Fancy network blocks: 1 mark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9yu-UZppOKh"
      },
      "source": [
        "## 1. Load and normalize the CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj5bzSfGocP0"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "1. Load and normalize CIFAR10\n",
        "Using ``torchvision``, it’s extremely easy to load CIFAR10.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "########################################################################\n",
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "# We transform them to Tensors of normalized range [-1, 1].\n",
        "\n",
        "########################################################################\n",
        "# .. note::\n",
        "#     If running on Windows and you get a BrokenPipeError, try setting\n",
        "#     the num_worker of torch.utils.data.DataLoader() to 0.\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1oLpkVSpd-s"
      },
      "source": [
        "## 2. Define a Convolutional Neural Network\n",
        "You can modify your network structure here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkWj1E54qkF_"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "##You can train on a GPU with the following code lines:\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#print(device)\n",
        "#net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Z0jRt8qlvw"
      },
      "source": [
        "## 3. Define a loss function and optimizer\n",
        "The baseline employs a classification cross-entropy loss and SGD with momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG-ju771q32A"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgR2Cln6rAtG"
      },
      "source": [
        "## 4. Train the network\n",
        "This is when things start to get interesting.\n",
        "We simply have to loop over our data iterator, and feed the inputs to the\n",
        "network and optimize.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHDzW_WxrR-F"
      },
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # Get the inputs; data is a list of [inputs, labels]\n",
        "        ###Train with GPU\n",
        "        #inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        ###Train with CPU\n",
        "        inputs, labels = data\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG3PMAhXryXE"
      },
      "source": [
        "\n",
        "\n",
        "## 5. Save the model and test the network on the test data\n",
        "We have trained the network for 2 passes over the training dataset.\n",
        "But we need to check if the network has learnt anything at all.\n",
        "\n",
        "We will check this by predicting the class label that the neural network\n",
        "outputs, and checking it against the ground-truth. If the prediction is\n",
        "correct, we add the sample to the list of correct predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2YMHwVQsUA1"
      },
      "source": [
        "####Save the trained weights\n",
        "#PATH = './cifar_net.pth'\n",
        "#torch.save(net.state_dict(), PATH)\n",
        "\n",
        "########################################################################\n",
        "# Next, let's load back in our saved model (note: saving and re-loading the model\n",
        "# wasn't necessary here, we only did it to illustrate how to do so):\n",
        "\n",
        "#net = Net()\n",
        "#net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "########################################################################\n",
        "# Let us look at how the network performs on the whole dataset.\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# Since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        ###Test on CPU\n",
        "        images, labels = data\n",
        "        ###Test on GPU\n",
        "        #images, labels = data[0].to(device), data[1].to(device)\n",
        "        # Calculate outputs by running images through the network \n",
        "        outputs = net(images)\n",
        "        # The class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}